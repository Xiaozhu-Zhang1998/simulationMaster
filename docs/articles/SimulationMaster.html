<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Guides • simulationMaster</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Guides">
<meta property="og:description" content="simulationMaster">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">simulationMaster</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/SimulationMaster.html">Guides</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Guides</h1>
            
      
      
      <div class="hidden name"><code>SimulationMaster.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="intoduction">1. Intoduction<a class="anchor" aria-label="anchor" href="#intoduction"></a>
</h2>
<p>This project builds a powerful Shiny App for people interested in statistical learning simulation, specifically for the regression task for sparse data and the classification task for data without linear decision boundary. We allow users to generate all different types of data and adjust model parameters, so they will have a direct understanding about their actions’ influences through the corresponding changes in resulting visualization graphs.</p>
<p>The way of this App doing regression simulation is inspired by the paper <em>Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons</em> (Hastie et al., 2020), where a systematic scheme for generating sparse data is provided and the effects of data structures on the performances of Ridge/LASSO/Elastic Net are carefully discussed. The performance here is a two-fold concept, regarding both prediction accuracy and feature selection ability, and the paper especially focus on the feature selection ability of these classical regularized models under different scenarios. For example, high correlation between features (especially between true features and null features) and low signal-noise-ratio lead to weak feature selection of LASSO. The permutation of true features and null features is also relevant in terms of feature selection consistency. These are sensible results coming from the so-called “sparse eigenvalue condition” and the “mutual incoherence condition” – though beyond the scope of this project, the simulation platform provided by this project would be a good way to validate these theoretical results from the practical side.</p>
<p>This projects particularly provides the user with privileges to tune relevant data generation parameters as well as model specification parameters of Ridge/LASSO/Elastic Net. The training MSE and cross-validated MSE will be plotted against a range of user-specified regularization parameters, and by cropping out an interested region more feature selection details about that region will be provided as well.</p>
<p>The classification simulation design of this project is inspired by the <a href="https://playground.tensorflow.org/" class="external-link"><em>Tensorflow Neural Network playground</em></a> (see Reference 1), which is an outstanding platform for tuning neural networks in order to fit data with complex structures. Although deep learning is known to be a powerful tool for classification, the SVM with all types of kernels is also a competitive algorithm. This project provides the user with options of 4 types of data structures, as well as 8 options of kernel types with their corresponding parameters. Compared with the Neural Network playground that inspired this project, one highlight and improvement of this project is that users are given more options to tune the data generation processes. In order to better visualize the distribution of data points and the decision boundary, here we only consider data with 2 dimensions and 2 classes.</p>
<p>The regression and classification tasks mentioned above are incorporated into this Shiny App through different tabs on the sidebar menu. Default parameters to generate classical model results have been provided as a beginner-friendly guide. After getting familiar with the playground’s basic operations, users can adjust the parameters for data generation and modeling by themselves. The results will be interactively and automatically generated, and visualization will be exhibited at the main panel accordingly.</p>
</div>
<div class="section level2">
<h2 id="implementation">2. Implementation<a class="anchor" aria-label="anchor" href="#implementation"></a>
</h2>
<div class="section level3">
<h3 id="regression-glmnet">2.1 Regression: Glmnet<a class="anchor" aria-label="anchor" href="#regression-glmnet"></a>
</h3>
<p>The regression task mainly focus on the data generation process and how regularization parameters affect the performances of penalized regression models. Using the <code>sim.xy</code> function in the <code>bestsubset</code> package, as well as the <code>cv.glmnet</code> function in the <code>glmnet</code> package, the users are allowed to generate data with desired parameters and conduct cross validation using Ridge/LASSO/Elastic net.</p>
<div class="section level4">
<h4 id="data-generation">2.1.1 Data Generation<a class="anchor" aria-label="anchor" href="#data-generation"></a>
</h4>
<p>The parameters of data generation include:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> (<code>Training set size</code>)</p></li>
<li><p><span class="math inline">\(nval\)</span> (<code>Test set size</code>)</p></li>
<li><p><span class="math inline">\(p\)</span> (<code>Number of features</code>)</p></li>
<li><p><span class="math inline">\(s\)</span> (<code>Number of nonzero coefficients</code>)</p></li>
<li><p><span class="math inline">\(\rho\)</span> (pairwise correlations of the predictors or <code>Correlation coefficient</code>)</p></li>
<li><p><span class="math inline">\(snr\)</span> (signal-to-noise or <code>SNR</code>), and</p></li>
<li><p><span class="math inline">\(beta.type\)</span> (pattern of the nonzero coefficients or <code>Beta Type</code>)</p></li>
</ul>
<p>We can define <span class="math inline">\(\beta\)</span> by specifying <span class="math inline">\(beta.type\)</span> and <span class="math inline">\(s\)</span>. There are 5 types of <span class="math inline">\(\beta\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Beta-type 1: <span class="math inline">\(\beta\)</span> has <span class="math inline">\(s\)</span> components equal to 1, occurring at roughly equally-spaced indices between 1 and <span class="math inline">\(p\)</span>, and the rest coefficients equal to 0;</p></li>
<li><p>Beta-type 2: <span class="math inline">\(\beta\)</span> has its first <span class="math inline">\(s\)</span> components equal to 1;</p></li>
<li><p>Beta-type 3: <span class="math inline">\(\beta\)</span> has its first <span class="math inline">\(s\)</span> components taking linear fashion nonzero values between 10 to 0.5;</p></li>
<li><p>Beta-type 4: <span class="math inline">\(\beta\)</span> has its first 6 components taking the nonzero values <span class="math inline">\(-10\)</span>, <span class="math inline">\(-6\)</span>, <span class="math inline">\(-2\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(6\)</span>, <span class="math inline">\(10\)</span>;</p></li>
<li><p>Beta-type 5: <span class="math inline">\(\beta\)</span> has its first <span class="math inline">\(s\)</span> components equal to 1, and the rest decaying to 0 at an exponential rate, i.e., <span class="math inline">\(\beta_i=0.5^{i-s}\)</span>, for <span class="math inline">\(i=s+1, \cdots , p\)</span>.</p></li>
</ol>
<p>Then the rows of design matrix <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span> can be independently and identically drawn from the Gaussian distribution <span class="math inline">\(\mathcal{N}(0, \Sigma)\)</span>, where the <span class="math inline">\(ij\)</span>-th entry of <span class="math inline">\(\Sigma\)</span> is <span class="math inline">\(\rho^{|i-j|}\)</span>. Next, the response variable can be generated from <span class="math inline">\(Y\sim\mathcal{N}(X\beta, \sigma^2I)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is defined according to the user-specified SNR, that is, <span class="math inline">\(\sigma^2=\frac{\beta^T \Sigma \beta }{snr}\)</span>.</p>
</div>
<div class="section level4">
<h4 id="regularized-regression-model-using-glmnet">2.1.2 Regularized Regression Model Using <code>glmnet</code><a class="anchor" aria-label="anchor" href="#regularized-regression-model-using-glmnet"></a>
</h4>
<p>We choose the function <code>cv.glmnet</code> to solve the following problem <span class="math display">\[
\min _{\beta_{0}, \beta} \frac{1}{N} \sum_{i=1}^{N} l\left(y_{i}, \beta_{0}+\beta^{T} x_{i}\right)+\lambda\left[(1-\alpha)\|\beta\|_{2}^{2} / 2+\alpha\|\beta\|_{1}\right]
\]</span></p>
<p>over a grid of user-specified values of <span class="math inline">\(\lambda\)</span> and the user-specified value of <span class="math inline">\(\alpha\)</span>, and conduct the procedure of k-fold cross-validation. Here <span class="math inline">\(l\left(y_{i}, \eta_{i}\right)\)</span> is the negative log-likelihood contribution for observation <span class="math inline">\(i\)</span>; e.g. for the Gaussian case it is <span class="math inline">\(\frac{1}{2}\left(y_{i}-\eta_{i}\right)^{2}\)</span>. The Elastic Net penalty is controlled by <span class="math inline">\(\alpha\)</span>, and bridges the gap between LASSO regression (<span class="math inline">\(\alpha=1\)</span>, the default) and ridge regression <span class="math inline">\((\alpha=0)\)</span>. The tuning parameter <span class="math inline">\(\lambda\)</span> controls the overall strength of the penalty. Users can tune these parameters of the model with <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> in the Shiny App.</p>
<p>The results obtained from <code>cv.glmnet</code> are used to do further analysis in terms of both prediction accuracy and feature selection. Details are provided in section 3.</p>
</div>
</div>
<div class="section level3">
<h3 id="classification-kernel-svm">2.2 Classification: Kernel SVM<a class="anchor" aria-label="anchor" href="#classification-kernel-svm"></a>
</h3>
<p>The classification task mainly focus on the generation of complex-structured data and how different kernels affect the classification ability of SVM. Using the <code>ksvm</code> function in the <code>kernlab</code> package, the users are allowed to perform the kernel SVM algorithm and see how the decision boundary would change in different scenarios.</p>
<div class="section level4">
<h4 id="data-generation-1">2.2.1 Data Generation<a class="anchor" aria-label="anchor" href="#data-generation-1"></a>
</h4>
<p>This project provides 4 options of data generation.</p>
<p><strong>Circle</strong></p>
<p>The Circle type of data represents a nested circular distribution. The outer ring consists of data of class 2, and the inner ring consists of data of class 1. The two coordinates <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> of a data point are independently and identically drawn from a standard Gaussian distribution. Then the label <span class="math inline">\(Y\)</span> of the data point can be determined using an intuitive method to decide the boundary – setting the radius of the inner circle as a quantile of <span class="math inline">\(\chi^2\)</span>. In other words, data points outside of the boundary are labeled with 2, and those in the circle are labeled with 1. In addition, noise is added in order to better evaluate the effectiveness of the classification methods. Each data point can be generated using the formula as follows: <span class="math display">\[X_1\sim^{\rm i.i.d} \mathcal{N}(0,1),\]</span> <span class="math display">\[X_2\sim^{\rm i.i.d} \mathcal{N}(0,1),\]</span> and <span class="math display">\[Y = \mathbb{I}\left( X_1^2 + X_2^2 + \mathcal{N}(0, \epsilon^2) &gt; \chi^2(p, df) \right) + 1,\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> indicates the <code>Noise</code> parameter in the App, <span class="math inline">\(p\)</span> indicates the <code>Prob. of Chi-square</code> parameter in the App, and <span class="math inline">\(df\)</span> indicates the <code>Df. of Chi-square</code> parameter in the App. Users could change the boundary by changing <span class="math inline">\(p\)</span> and <span class="math inline">\(df\)</span>, and control the perturbation via the value of <span class="math inline">\(\epsilon^2\)</span>.</p>
<p><strong>XOR</strong></p>
<p>The XOR type of data is similar to the first one except the way the class labels being determined. When only one of the two coordinates (<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) is positive or negative, the label <span class="math inline">\(Y\)</span> will be 2, otherwise 1. In other words, data points in the first and the third quadrants have the same labels, while data points in the second and the fourth quadrant share the same labels. Similarly, noise is added to the data. Each data point can be generated using the formula as follows:</p>
<p><span class="math display">\[X_1\sim^{\rm i.i.d} \mathcal{N}(0,\sigma^2),\]</span> <span class="math display">\[X_2\sim^{\rm i.i.d} \mathcal{N}(0,\sigma^2),\]</span> and <span class="math display">\[Y = XOR \left( X_1 &gt; \mathcal{N}(0, \epsilon^2), \ X_2 &gt; \mathcal{N}(0, \epsilon^2) \right) + 1.\]</span> Note that <span class="math inline">\(\sigma\)</span> indicates the <code>Variability of data</code> parameter in the App, and <span class="math inline">\(\epsilon\)</span> indicates the <code>Noise</code> parameter in the App.</p>
<p><strong>Gaussian</strong></p>
<p>The Gaussian type of data represents two groups of bivariate Gaussian distributions. Each data point can be generated using the formula as follows. For class 1, <span class="math display">\[
\begin{bmatrix}
X_1 \\ X_2
\end{bmatrix}
\sim^{\rm i.i.d}
\mathcal{N}\left( \begin{bmatrix}\mu_{11} \\ \mu_{12} \end{bmatrix},
\begin{bmatrix} \sigma_{11}^2 &amp; \rho_1 \sigma_{11}  \sigma_{12}   \\
\rho \sigma_{11} \sigma_{12}  &amp; \sigma_{12}^2 \end{bmatrix}
 \right).
\]</span> For class 2, <span class="math display">\[
\begin{bmatrix}
X_1 \\ X_2
\end{bmatrix}
\sim^{\rm i.i.d}
\mathcal{N}\left( \begin{bmatrix}\mu_{21} \\ \mu_{22} \end{bmatrix},
\begin{bmatrix} \sigma_{21}^2 &amp; \rho_2 \sigma_{21}  \sigma_{22}   \\
\rho \sigma_{21} \sigma_{22}  &amp; \sigma_{22}^2 \end{bmatrix}
 \right).
\]</span> Note that <span class="math inline">\(\mu_{11}\)</span> indicates the <code>Mu1 of class 1</code> parameter in the App, <span class="math inline">\(\mu_{12}\)</span> indicates the <code>Mu2 of class 1</code> parameter in the App, <span class="math inline">\(\mu_{21}\)</span> indicates the <code>Mu1 of class 2</code> parameter in the App, and <span class="math inline">\(\mu_{22}\)</span> indicates the <code>Mu2 of class 2</code> parameter in the App. In addition, <span class="math inline">\(\sigma_{11}\)</span> indicates the <code>Sigma1 of class 1</code> parameter in the App, <span class="math inline">\(\sigma_{12}\)</span> indicates the <code>Sigma2 of class 1</code> parameter in the App, <span class="math inline">\(\sigma_{21}\)</span> indicates the <code>Sigma1 of class 2</code> parameter in the App, <span class="math inline">\(\sigma_{22}\)</span> indicates the <code>Sigma2 of class 2</code> parameter in the App, <span class="math inline">\(\rho_1\)</span> indicates the <code>Rho of class 1</code> parameter in the App, and <span class="math inline">\(\rho_2\)</span> indicates the <code>Rho of class 2</code> parameter in the App.</p>
<p><strong>Spiral</strong></p>
<p>The Spiral type of data represents two intertwined spiral distributions. Each class of data points can be generated by specifying the polar coordinates of the start and end points, and then specifying other points with equally-spaced polar coordinates in-between. Similarly, noise is added to the data. For class 1, the start point is</p>
<p><span class="math display">\[
\left( X_1, X_2 \right) = \left(r \cdot \cos(t_{11}) + \mathcal{N}(0, \epsilon^2),\ r \cdot \sin(t_{11}) + \mathcal{N}(0, \epsilon^2) \right),
\]</span> and the end point is <span class="math display">\[
\left( X_1, X_2 \right) = \left(r \cdot \cos(t_{12}) + \mathcal{N}(0, \epsilon^2),\ r \cdot \sin(t_{12}) + \mathcal{N}(0, \epsilon^2) \right).
\]</span></p>
<p>For class 2, the start point is <span class="math display">\[
\left( X_1, X_2 \right) = \left(r \cdot \cos(t_{21}) + \mathcal{N}(0, \epsilon^2),\ r \cdot \sin(t_{21}) + \mathcal{N}(0, \epsilon^2) \right),
\]</span></p>
<p>and the end point is <span class="math display">\[
\left( X_1, X_2 \right) = \left(r \cdot \cos(t_{22}) + \mathcal{N}(0, \epsilon^2),\ r \cdot \sin(t_{22}) + \mathcal{N}(0, \epsilon^2) \right).
\]</span></p>
<p>Note that <span class="math inline">\(r\)</span> indicates the <code>Radius</code> parameter in the App, <span class="math inline">\(\epsilon\)</span> indicates the <code>Noise</code> parameter in the App, <span class="math inline">\(t_{11}\)</span> and <span class="math inline">\(t_{12}\)</span> indicate the start and end values of the <code>Angle of class 1</code> parameter in the App, and <span class="math inline">\(t_{21}\)</span> and <span class="math inline">\(t_{22}\)</span> indicate the start and end values of the <code>Angle of class 2</code> parameter in the App.</p>
</div>
<div class="section level4">
<h4 id="kernel-svm-model">2.2.2 Kernel SVM Model<a class="anchor" aria-label="anchor" href="#kernel-svm-model"></a>
</h4>
<p>We choose the <code>ksvm</code> function in the <code>kernlab</code> package to conduct the kernel soft-margin SVM algorithm. The (dual) optimization problem is: <span class="math display">\[\begin{gather*}
\max_{w, \alpha} \quad W(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j}^n y^{(i)}y^{(j)} \alpha_i \alpha_j k(x^{(i)}, x^{(j)}) \\
{\rm s.t.} \quad 0 \leq \alpha_i \leq C, \quad i = 1,\cdots,n \\
\quad \quad \sum_{i=1}^n \alpha_i y^{(i)} = 0.
\end{gather*}\]</span></p>
<p>The <span class="math inline">\(C\)</span> here indicates the <code>Cost</code> parameter in the App, and types of <span class="math inline">\(k(x^{(i)}, x^{(j)})\)</span> indicates the <code>Kernel</code> parameter in the App. The kernel types this App provides include: RBF, Polynomial, Linear, Bessel, Laplacian, Spline, ANOVA RBF, and Hyperbolic tangent. Their corresponding expressions and parameters are:</p>
<ul>
<li><p>Gaussian RBF kernel: <span class="math inline">\(k(x,x') = \exp(-\sigma\begin{Vmatrix}x-x'\end{Vmatrix}^2)\)</span></p></li>
<li><p>Polynomial kernel: <span class="math inline">\(k(x,x') = (scale&lt;x,x'&gt;+offset)^{degree}\)</span></p></li>
<li><p>Linear kernel: <span class="math inline">\(k(x,x') = &lt;x,x'&gt;\)</span></p></li>
<li><p>Hyperbolic tangent kernel: <span class="math inline">\(k(x,x') = \tanh(scale&lt;x,x'&gt;+offset)\)</span></p></li>
<li><p>Laplacian kernel: <span class="math inline">\(k(x,x') = \exp(-\sigma\begin{Vmatrix}x - x'\end{Vmatrix})\)</span></p></li>
<li><p>Bessel kernel: <span class="math inline">\(k(x,x') = (-Bessel_{(\nu+1)}^n\sigma\begin{Vmatrix}x-x'\end{Vmatrix}^2)\)</span></p></li>
<li><p>ANOVA RBF kernel: <span class="math inline">\(k(x,x') = \sum_{1\leq i_1 \ldots &lt;i_D \leq N}\prod_{d=1}^Dk(x_{id},x_{id}')\)</span> where <span class="math inline">\(k(x,x)\)</span> is a Gaussian RBF kernel.</p></li>
<li><p>Spline kernel: <span class="math inline">\(k(x,x') = \prod_{d=1}^D 1 + x_i x_j + x_i x_j \min(x_i,x_j) - \frac{x_i+x_j}{2} \min(x_i,x_j)^2 + \frac{\min(x_i,x_j)^3}{3}\)</span></p></li>
</ul>
</div>
</div>
</div>
<div class="section level2">
<h2 id="demonstration">3. Demonstration<a class="anchor" aria-label="anchor" href="#demonstration"></a>
</h2>
<div class="section level3">
<h3 id="package-dependency-and-installation">3.1 Package Dependency and Installation<a class="anchor" aria-label="anchor" href="#package-dependency-and-installation"></a>
</h3>
<p>Before installing the package, the user needs to first install a package called <code>bestsubset</code> by running the following code.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://devtools.r-lib.org/" class="external-link">devtools</a></span><span class="op">)</span>
<span class="fu"><a href="https://devtools.r-lib.org/reference/remote-reexports.html" class="external-link">install_github</a></span><span class="op">(</span>repo <span class="op">=</span> <span class="st">"ryantibs/best-subset"</span>, subdir <span class="op">=</span> <span class="st">"bestsubset"</span><span class="op">)</span></code></pre></div>
<p>Then the user can install and load this package.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://devtools.r-lib.org/reference/remote-reexports.html" class="external-link">install_github</a></span><span class="op">(</span>repo <span class="op">=</span> <span class="st">"Sta523-Fa21/final_proj_simulation_master"</span>, 
               subdir <span class="op">=</span> <span class="st">"simulationMaster"</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">simulationMaster</span><span class="op">)</span></code></pre></div>
<p>By running the following function, the user opens the Shiny App.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/run_simulation_master.html">run_simulation_master</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>After opening this Shiny App, the user can choose whether to do a regression or classification task on the left sidebar menu.</p>
</div>
<div class="section level3">
<h3 id="regression-demo">3.2 Regression Demo<a class="anchor" aria-label="anchor" href="#regression-demo"></a>
</h3>
<p>For regression with <code>cv.glmnet</code>, <strong>data generation parameters</strong> and <strong>model specification parameters</strong> can be tuned and adjusted freely (see Figure 1). We allow the user to select a large bunch of <span class="math inline">\(\lambda\)</span>’s at one time. Due to the effects of <span class="math inline">\(\lambda\)</span> of different magnitudes, we choose log(<span class="math inline">\(\lambda\)</span>) rather than <span class="math inline">\(\lambda\)</span> itself, and we set a lower bound and upper bound for the value of log(<span class="math inline">\(\lambda\)</span>). Then, our function will generate desired number of equally-spaced <span class="math inline">\(\lambda\)</span> values within this range and fit the model. The corresponding results will be shown below accordingly.</p>
<div class="figure">
<img src="../../../Github%20Repo/simulationMaster/articles/images/pic1.png" style="width:100.0%" alt=""><p class="caption">Figure 1. Tuning space for regression</p>
</div>
<p>The result panels for this regression task are displayed in Figure 2. The user is able to see the <strong>training MSE</strong> and the <strong>cross-validation MSE</strong> with standard error bars in the left panel of the plot. Specifically, the user can select a region on this plot by cropping out a rectangle, and then the corresponding <strong>feature selection</strong> results, which include a statistics table and 4 feature selection plots (see Figure 3), will show in the panel at the bottom.</p>
<p>In Figure 2, the black vertical line in the plot represents the <span class="math inline">\(\log(\lambda)\)</span> achieving minimum <strong>validation MSE</strong>, and the red line represents the <span class="math inline">\(\log(\lambda)\)</span> achieving <strong>one-standard error MSE</strong>. In Figure 3, 4 models are considered: the top left plot represents the model under the “black line” <span class="math inline">\(\lambda\)</span>, the top right plot represents the model under the “red line” <span class="math inline">\(\lambda\)</span>, the bottom left plot represents the model under the minimal <span class="math inline">\(\lambda\)</span> that the user chooses, and the bottom right plot represents the model under the maximal <span class="math inline">\(\lambda\)</span> that the user chooses. We can see that larger <span class="math inline">\(\lambda\)</span> leads to more sparse solution. The statistics regarding feature selection results are also displayed (see Figure 2), including True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN), True Positive Rate (TPR), False Positive Rate (FPR), Accuracy, Precision, False Discovery Rate, and F-score of the 4 interested models.</p>
<p>The right panel of Figure 2 is a summary of the regression. With the interactive and straightforward results shown in these panels, users will be able to quickly grasp the information they need and further tune and compare the regression models.</p>
<div class="figure">
<img src="../../../Github%20Repo/simulationMaster/articles/images/pic2.png" style="width:100.0%" alt=""><p class="caption">Figure 2. Results for regression</p>
</div>
<div class="figure">
<img src="../../../Github%20Repo/simulationMaster/articles/images/pic3.png" style="width:100.0%" alt=""><p class="caption">Figure 3. Feature selection plots for regression</p>
</div>
</div>
<div class="section level3">
<h3 id="classification-demo">3.2 Classification Demo<a class="anchor" aria-label="anchor" href="#classification-demo"></a>
</h3>
<p>For classification with kernel SVM, similar to the regression playground, the tuning panels include two parts, <strong>data generation</strong> and <strong>model specification</strong> (see Figure 4). Users can choose different data types and other data generation parameters on the left panel and choose a specific kernel and its corresponding parameters for kernel SVM on the right panel.</p>
<div class="figure">
<img src="../../../Github%20Repo/simulationMaster/articles/images/pic4.png" style="width:100.0%" alt=""><p class="caption">Figure 4. Tuning space for classification</p>
</div>
<p>In the result panels (see Figure 5), the visualization is shown in the left box and the summary statistics in the right box. The summary statistics include the following terms: objective function values, training error, cross validation error, test error, and number of support vectors.</p>
<p>For the visualization, we paint our data points in two different colors to show their labels, and naturally the decision regions are also painted in the corresponding light colors. We achieve this by dividing the whole space into thousands of grids and coloring them with the color of the predicted class. In this way, users can intuitively perceive the change of classification results of kernel SVM.</p>
<div class="figure">
<img src="../../../Github%20Repo/simulationMaster/articles/images/pic5.png" style="width:100.0%" alt=""><p class="caption">Figure 5. Results for classification</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Tensorflow Neural Network playground: <a href="https://playground.tensorflow.org/" class="external-link"><code>https://playground.tensorflow.org/</code></a></p></li>
<li><p>Trevor Hastie, Robert Tibshirani and Ryan Tibshirani (2020). Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons. Statistical Science, Vol. 35, No. 4, 579–592. <a href="https://doi.org/10.1214/19-STS733" class="external-link"><code>https://doi.org/10.1214/19-STS733</code></a></p></li>
<li><p>Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. <a href="https://www.jstatsoft.org/v33/i01/" class="external-link"><code>https://www.jstatsoft.org/v33/i01/</code></a></p></li>
<li><p>Alexandros Karatzoglou, Alex Smola, Kurt Hornik, Achim Zeileis (2004). kernlab - An S4 Package for Kernel Methods in R. Journal of Statistical Software 11(9), 1-20. <a href="http://www.jstatsoft.org/v11/i09/" class="external-link"><code>http://www.jstatsoft.org/v11/i09/</code></a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Xiaozhu Zhang, Xinran Song, Tong Lin, Xuyang Tian.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.1.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
